<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI" />
        <title>Yana Hasson</title>

        <link rel='icon' href='favicon.ico' type='image/x-icon'/ >
        <link href="./css/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="./assets/academicons-1.7.0/css/academicons.css"/>
        <link rel="stylesheet" href="./assets/font-awesome-4.7.0/css/font-awesome.min.css"/>
	<script async defer src="https://buttons.github.io/buttons.js"></script>
    </head>

    <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">
          
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">
             
            <ul class="nav navbar-nav navbar-left">
              <li ><a href="#news">News</a></li>
              <li ><a href="#research">Research</a></li>
              <li ><a href="#code">Code</a></li>
            </ul>
          </div>
        </div>
      </div> 

      <!-- end of navigation bar -->

      <div style="height:80px;"></div>
      <div style="height:40px;"></div>

      <!-- CONTENTS -->
      <div class="container">
        <!-- Aboutme -->
        <!--<div id="aboutme"></div> -->
        <div class="row" >
          <div class="col-xs-6 col-sm-4 col-md-2">
            <a class="thumbnail">
              <img src="./img/yana.png" alt="Yana Hasson" class="img-rounded">
            </a>            
          </div>

          <div class="col-xs-10 col-sm-6 col-md-4">
            <h1 class="text-info">Yana Hasson</h1>
            <h4 class="text-info">PhD student, INRIA</h4>
            <h5>
              <a href="mailto:yana.hasson.inria@gmail.com" class="text-info" title="e-Mail"><i class="fa fa-envelope-square fa-2x"></i></a>
	      <a href="https://scholar.google.com/citations?user=yhz7sFoAAAAJ" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a>
              <a href="https://github.com/hassony2" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a>
              <a href="https://www.linkedin.com/in/yana-hasson-5a391774/" class="text-info" title="LinkedIn"><i class="fa fa-linkedin-square fa-2x"></i></a>
              <a href="https://twitter.com/yanahasson" class="text-info" title="Twitter"><i class="fa fa-twitter-square fa-2x"></i></a>
            </h5>
          </div>
        </div> <!-- end of Aboutme -->

         <p align="justify">
	 I am a Research Scientist at <a href="https://deepmind.com/"> DeepMind </a>. Before that, I was a PhD student in the computer vision and machine learning research laboratory (<a href="http://www.di.ens.fr/willow">WILLOW project team</a>) in the  <a href="http://www.di.ens.fr">Department of Computer Science</a> of <a href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure (ENS)</a> and in <a href="http://www.inria.fr/">Inria Paris</a> where I worked on understanding first person videos under the supervision of <a href="http://www.di.ens.fr/~laptev">Ivan Laptev</a> and <a href="http://lear.inrialpes.fr/~schmid/">Cordelia Schmid</a>.
	 I have received a MS degree in Applied Mathematics from <a href="http://www.centralesupelec.fr/">École Centrale Paris </a> and a MS degree in <a href="http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/">Mathematics, Vision and Learning</a> from <a href="http://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
            </p>
        <hr>

        <!-- News header-->
        <div class="row" id="news" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>News</h2>
          </div>
        </div>
        <!-- End news header -->
	      
<!-- DeepMind -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">11 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
		  I joined <a href="https://deepmind.com/"> DeepMind </a> as a Research Scientist.
	  </div>
        </div>
<!-- PhD defense -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">10 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
		  I defended my PhD !
	  </div>
        </div>
<!-- CEMRACS -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">08 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
		I am attending the 2021 <a href="http://smai.emath.fr/cemracs/cemracs21/projects_list.html">CEMRACS</a> summer program on Data Assimilation and Reduced Modeling for High Dimensional Problems in Luminy.  
	  </div>
        </div>
<!-- homan -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">08 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
		  New <a href="https://arxiv.org/abs/2108.07044">paper</a> on arXiv on <a href="https://hassony2.github.io/homan.html">joint hand-object fitting</a> to noisy evidence in short RGB clips.
	  </div>
        </div>
<!-- Robotics talk 2020 -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
		  I gave a <a href="https://www.youtube.com/watch?v=gAqIngwZN0E">talk</a> to present our work on hand-object reconstruction at the <a href="https://www.pair.toronto.edu/robotics-rg/"> CS Robotics Reading group, at the University of Toronto </a>.
          </div>
        </div>
  <!-- ECCV 2020 -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">08 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            We held the <a href="https://sites.google.com/corp/view/wicvworkshop-eccv2020/">WiCV workshop</a> at <a href="https://eccv2020.eu/">ECCV'20</a> !
          </div>
        </div>
  <!-- Facebook internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">08 / 2020 </span>
          </div>
          <div class="col-sm-11 col-md-11">
		  I completed a 3-month research internship at <a href="https://ai.facebook.com/">Facebook AI Research </a> in Paris
          </div>
        </div>
  <!-- Photometric consistency code release -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">06 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="handobjectconsist.html"> CVPR'20 paper </a> <a href="https://github.com/hassony2/handobjectconsist"> code </a> released
          </div>
        </div>
  <!-- Google internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">06 / 2020 </span>
          </div>
          <div class="col-sm-11 col-md-11">
            I completed a 6-month research internship at Google
          </div>
        </div>
<!-- Photometric consistency accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="handobjectconsist.html"> CVPR'20 paper </a> accepted. Code coming soon !
          </div>
        </div>
<!-- MSR talk -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">10 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
		  I presented at the <a href="https://www.microsoft.com/en-us/research/event/mixed-reality-and-ai-zurich-lab-launch/#!agenda"> Mixed Reality & AI Zurich Lab Workshop</a> during my internship at <a href="https://www.microsoft.com">Microsoft</a>.
          </div>
        </div>

	<!-- Obman accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">02 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
            <a href="obman.html">CVPR'19 paper</a> accepted on hand-object reconstruction!
          </div>
        </div>
        <div style="height:3px;"></div>

        <div style="height:3px;"></div>

        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">09 / 2018</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I coorganized the 5th <a href="https://wicvworkshop.github.io/ECCV2018/index.html"> WiCV workshop</a> which took place in conjunction with <a href="https://eccv2018.org">ECCV'18</a> in Munich.
          </div>
        </div>
        <div style="height:3px;"></div>

        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-primary">04 / 2018</span>
          </div>
          <div class="col-sm-11 col-md-11">
		  I visited the <a href="https://ps.is.tuebingen.mpg.de/"> Perceiving Systems </a> team at <a href="https://ps.is.tuebingen.mpg.de/">MPI</a> for a month.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow  -->

        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">11 / 2017</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I started my PhD at <a href="http://www.di.ens.fr/willow">WILLOW</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow internship -->

        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">05 / 2017</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I joined <a href="http://www.di.ens.fr/willow">WILLOW project team</a> as a research intern!
          </div>
        </div>
        <div style="height:3px;"></div>

        <!-- end of news -->
        <hr>

        <!-- Research -->
        <div class="row"  id="research" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Research</h2>
		  
		  

<!-- HOMAN 2021 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/homanteaser.png" alt="Towards unconstrained joint hand-object reconstruction from RGB videos">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
        <strong>Towards unconstrained joint hand-object reconstruction from RGB videos</strong> </br>
	<u>Yana Hasson</u>, Gül Varol, Ivan Laptev and Cordelia Schmid<br>
	<em><a href="https://3dv2021.surrey.ac.uk/">3DV</a>, </em> 2021.<br>
	<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
	<a href="homan.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex6">bibtex</button>
	<div id="bibtex6" class="collapse">
	  <pre><tt>@article{{hasson20_homan,
		  title &nbsp&nbsp&nbsp&nbsp= {Towards unconstrained joint hand-object reconstruction from RGB videos},
		  author &nbsp&nbsp = {Hasson, Yana and Varol, G"{u}l and Laptev, Ivan and Schmid, Cordelia},
                  journal &nbsp={arXiv preprint arXiv:2108.07044},
		  year &nbsp&nbsp&nbsp&nbsp&nbsp= {2021}
	}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
	<div id="abstract6" class="collapse">
        <p style="text-align: justify;">
	Our work aims to obtain 3D reconstruction of hands and manipulated objects from monocular videos.
		Reconstructing hand-object manipulations holds a great potential for robotics and learning from human demonstrations.
		The supervised learning approach to this problem, however, requires 3D supervision and remains limited to constrained laboratory settings and simulators for which 3D ground truth is available.
		In this paper we first propose a learning-free fitting approach for hand-object reconstruction which can seamlessly handle two-hand object interactions.
		Our method relies on cues obtained with common methods for object detection, hand pose estimation and instance segmentation.
		We quantitatively evaluate our approach and show that it can be applied to datasets with varying levels of difficulty for which training data is unavailable.
	</p>
        </div>
      </div>
    </div>
<!-- end of HOMAN  -->
	  
<!-- Low bandwidth -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/lowbandwidth.jpg" alt="Leveraging Phtometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction">
                </a>
	</div>

        <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Low Bandwidth Video-Chat Compression using Deep Generative Models</strong> </br>
	  Maxime Oquab, Pierre Stock, Oran Gafni, Daniel Haziza, Tao Xu, Peizhao Zhang, Onur Celebi, <u>Yana Hasson</u>, Patrick Labatut, Bobo Bose-Kolanu, Thibault Peyronnel, Camille Couprie<br>


	<a href="https://arxiv.org/abs/2012.00328"><button type="button" class="btn btn-primary btn-xs">paper</button></a>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract7">abstract</button>
	<div id="abstract7" class="collapse">
        <p style="text-align: justify;">
	To unlock video chat for hundreds of millions of people hindered by poor connectivity or unaffordable data costs, we propose to authentically reconstruct faces on the receiver's device using facial landmarks extracted at the sender's side and transmitted over the network.
	In this context, we discuss and evaluate the benefits and disadvantages of several deep adversarial approaches.
	In particular, we explore quality and bandwidth trade-offs for approaches based on static landmarks, dynamic landmarks or segmentation maps.
	We design a mobile-compatible architecture based on the first order animation model of Siarohin et al.
	In addition, we leverage SPADE blocks to refine results in important areas such as the eyes and lips.
	We compress the networks down to about 3MB, allowing models to run in real time on iPhone 8 (CPU).
	This approach enables video calling at a few kbits per second, an order of magnitude lower than currently available alternatives.
	</p>
        </div>

      </div>
    </div>
<!-- end of Low bandwidth -->

<!-- Photometric consistency, CVPR'20 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/teaser_cvpr20.png" alt="Leveraging Phtometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
        <strong>Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction</strong> </br>
	<u>Yana Hasson</u>, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys, and Cordelia Schmid<br>
	<em>CVPR, </em> 2020.<br>
	<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
	<a href="handobjectconsist.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex6">bibtex</button>
	<div id="bibtex6" class="collapse">
	  <pre><tt>@INPROCEEDINGS{hasson20_handobjectconsist,
		  title &nbsp&nbsp&nbsp&nbsp= {Leveraging Photometric Consistency over Time for Sparsely Supervised Hand-Object Reconstruction},
		  author &nbsp&nbsp = {Hasson, Yana and Tekin, Bugra and Bogo, Federica and Laptev, Ivan and Pollefeys, Marc and Schmid, Cordelia},
		  booktitle = {CVPR},
		  year &nbsp&nbsp&nbsp&nbsp&nbsp= {2020}
	}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
	<div id="abstract6" class="collapse">
        <p style="text-align: justify;">
	Modeling hand-object manipulations is essential for understanding how humans interact with their environment.
	While of practical importance, estimating the pose of hands and objects during interactions is challenging due to the large mutual occlusions that occur during manipulation.
	Recent efforts have been directed towards fully-supervised methods that require large amounts of labeled training samples. Collecting 3D ground-truth data for hand-object interactions, however, is costly, tedious, and error-prone. To overcome this challenge we present a method to leverage photometric consistency across time when annotations are only available for a sparse subset of frames in a video.
	Our model is trained end-to-end on color images to jointly reconstruct hands and objects in 3D by inferring their poses.
	Given our estimated reconstructions, we differentiably render the optical flow between pairs of adjacent images and use it within the network to warp one frame to another.
	We then apply a self-supervised photometric loss that relies on the visual consistency between nearby images.
	We achieve state-of-the-art results on 3D hand-object reconstruction benchmarks and demonstrate that our approach allows us to improve the pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.
	</p>
        </div>
      </div>
    </div>
<!-- end of CVPR'20 -->


<!-- ObMan, CVPR'19 -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/obman.png" alt="Learning joint reconstruction of hands and manipulated objects">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>Learning joint reconstruction of hands and manipulated objects</strong><br>
		<u>Yana Hasson</u>, G&uuml;l Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, and Cordelia Schmid<br>
                <em>CVPR, </em> 2019.<br>
		<!--<a href=""> <button type="button" class="btn btn-primary btn-xs"> pdf </button> </a> -->
		<a href="obman.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex5">bibtex</button>
                <div id="bibtex5" class="collapse">
                  <pre><tt>@INPROCEEDINGS{hasson19_obman,
  title &nbsp&nbsp&nbsp&nbsp= {Learning joint reconstruction of hands and manipulated objects},
  author &nbsp&nbsp = {Hasson, Yana and Varol, G{\"u}l and Tzionas, Dimitrios and Kalevatykh, Igor and Black, Michael J. and Laptev, Ivan and Schmid, Cordelia},
  booktitle = {CVPR},
  year &nbsp&nbsp&nbsp&nbsp&nbsp= {2019}
}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract5">abstract</button>
                <div id="abstract5" class="collapse">
		  <div>
Estimating hand-object manipulations is essential for interpreting and imitating human actions.
Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation.
Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object.
While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations.
For example, during manipulation, the hand and object should be in contact but not interpenetrate.
In this work we regularize the joint reconstruction of
hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a
novel contact loss that favors physically plausible hand-object constellations.
To train and evaluate the model, we also propose a new large-scale synthetic
dataset, ObMan, with hand-object manipulations.  Our approach significantly
improves grasp quality metrics over baselines on synthetic and real datasets,
using RGB images as input.
                  </div>
                </div>
              </div>
            </div>
<!-- end of ObMan -->

          </div>
        </div> <!-- end of projects -->

        <!-- Code -->
        <div class="row"  id="code" style="padding-top:60px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Code</h2>

	    <div class="row"  style="padding-top:60px; margin-top:-60px;">
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/manopth"> manopth </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/manopth" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/manopth on GitHub">Star</a>
			</br>
			Port of <a href="http://mano.is.tue.mpg.de/"> MANO </a> differentiable hand as a <a href="https://pytorch.org">PyTorch</a> differentiable layer.
		    </div>


		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/kinetics_i3d_pytorch"> kinetics_i3d_pytorch </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/kinetics_i3d_pytorch" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/kinetics_i3d_pytorch on GitHub">Star</a>
			</br>
			    Port of I3D network for action recognition to PyTorch.
			    Transfer of weights trained on <a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/"> Kinetics </a> dataset.

		    </div>

		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/torch_videovision"> torch_videovision </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/torch_videovision" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/torch_videovision on GitHub">Star</a>
			</br>
			Utilities for video data-augmentation.
		    </div>
	    </div>
	    <div class="row"  style="padding-top:20px; margin-top:0px;">
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/inflated_convnets_pytorch"> inflated_convnets_pytorch </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/inflated_convnets_pytorch" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/inflated_convnets_pytorch on GitHub">Star</a>
			</br>
			    Inflation from image input to video inputs of ResNets and DenseNets.
			    Weights initialized based on ImageNet.


		    </div>
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/useful-computer-vision-phd-resources"> useful-computer-vision-phd-resources </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/useful-computer-vision-phd-resources" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/useful-computer-vision-phd-resources">Star</a>
			</br>
			Some useful tips and resources for PhDs in computer vision
		    </div>

		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/handobjectconsist"> handobjectconsist </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/handobjectconsist" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/handobjectconsist on GitHub">Star</a>
			</br>
			[cvpr20] Demo and training code for sparsely-supervised Hand-Object reconstruction with photometric supervision
		    </div>
	   </div>
	    <div class="row"  style="padding-top:20px; margin-top:0px;">
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/obman_train"> obman_train </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/obman_train" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/obman_train on GitHub">Star</a>
			</br>
			[cvpr19] Demo, evaluation and training code for Hand-Object reconstruction
		    </div>


		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/obman"> obman </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/obman" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/obman on GitHub">Star</a>
			</br>
			Load the <a href="https://www.di.ens.fr/willow/research/obman/data/"> ObMan dataset </a>

		    </div>
		     <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/obman_render"> obman_render </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/obman_render" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/obman_render on GitHub">Star</a>
			</br>
			Code to render the <a href="https://www.di.ens.fr/willow/research/obman/data/"> ObMan dataset </a>

		    </div>

	    </div>

	    <div class="row"  style="padding-top:20px; margin-top:0px;">
		    <div class="col-xs-10 col-sm-4 col-md-4 card-body card">
		    <a href="https://github.com/hassony2/homan"> homan </a> 
			</br>
		    <a class="github-button" href="https://github.com/hassony2/homan" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star hassony2/homan on GitHub">Star</a>
			</br>
			 Hand-Object joint fitting to noisy evidence
		    </div>

	    </div>
	   </div>
	</div>
      </div> 
      <hr>

      <div class="container">
        <footer>
          <p align="right"><small>Copyright © Yana Hasson &nbsp;/&nbsp; Last update:August 2021</small></p>
        </footer>
        <div style="height:10px;"></div>
      </div>

      <!-- Bootstrap core JavaScript -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="jq/jquery-1.11.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script src="js/docs.min.js"></script>

      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-75922772-1', 'auto');
        ga('send', 'pageview');

      </script>
    </body>
</html>


